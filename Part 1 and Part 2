# -*- coding: utf-8 -*-
"""Part1_&_Part2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s-3lzLeRxhdMop8RXh_W0r8r28lbnMOE

**Google Drive has to be mounted to this colab ntebook to access the dataset**
"""

#Command to Mount google drive to this notebook
from google.colab import drive
drive.mount('/content/drive')

"""**Load the Required libraries**"""

#IMporting requried libraries
import pandas as pd #pandas to read csv file 
import numpy as np #numpy to import array function
from numpy import array #Convert dataframe rows to array
import tensorflow as tf #To build deep learning models
from tensorflow import keras #To build deep learning layers
from sklearn.feature_extraction.text import CountVectorizer 
from keras.preprocessing.text import Tokenizer #Tokenize the reviews
from keras.preprocessing.sequence import pad_sequences #Pad the reviews 
from tensorflow.keras import models, layers, optimizers, losses, metrics #To build tensorflow modeels 
from keras.layers.merge import concatenate #To merge the output of the convolutional layers
from keras.models import Sequential,Model #Build keras sequential model and functional model
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D,SimpleRNN,Conv1D,GlobalMaxPooling1D,Input #Build various different layers
from keras.layers import Dropout, Activation, Flatten, GlobalAveragePooling1D,AveragePooling1D,MaxPooling1D #Build variour layers and add regularizations
from sklearn.model_selection import train_test_split #Split dataset to training, validation and test set
from keras.utils.np_utils import to_categorical
import re

#Create early stoppping when loss of validation becomes more than train
from tensorflow.keras.callbacks import EarlyStopping 
stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1)

#Moduels from pydrive to save and load models to google drive
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive 
from google.colab import auth 
from oauth2client.client import GoogleCredentials

#Google drive is authenticated to have an access to store models to the drive
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

"""# **Part 1: IMDB Modelling Task**

**Function Definations for plotting accuracy and loss plots**
"""

#Function for plotting Train vs validate accuracy plots
def accuracy_plot(model_summary): #Function defination
  from matplotlib import pyplot as plt #import pyplot to plot charts
  plt.plot(model_summary.history['accuracy']) #From history of the fit model access training accuracy
  plt.plot(model_summary.history['val_accuracy']) #From history of the fit model access validation accuracy 
  plt.title('model accuracy') #Titel of the plot
  plt.ylabel('accuracy') #Lable y-axis 
  plt.xlabel('epoch') #Lable x-axis
  plt.legend(['train', 'validation'], loc='upper left') #Define legends for the two line's in the plot and display the legend to upper left corner 
  plt.show() #Display the plot

#Function for plotting Train vs validate loss plots
def loss_plot(model_summary): #Function defination
  from matplotlib import pyplot as plt #import pyplot to plot charts
  plt.plot(model_summary.history['loss']) #From history of the fit model access training loss
  plt.plot(model_summary.history['val_loss']) #From history of the fit model access validation loss 
  plt.title('model loss') #Titel of the plot
  plt.ylabel('loss') #Lable y-axis 
  plt.xlabel('epoch') #Lable x-axis
  plt.legend(['train', 'validation'], loc='upper left') #Define legends for the two line's in the plot and display the legend to upper left corner 
  plt.show() #Display the plot

"""**Import the dataset from Google drive**"""

#Import dataset from drive 
data= pd.read_csv('/content/drive/My Drive/IMDB Dataset.csv') #Read file from the path in google drive where it is stored and store it in pandas dataframe
imdb = data.iloc[:50000,:] #To select all the rows of the dataframe
imdb.head(10) #Display the contents of first 10 rows

#Display number of positive and negative reviews
print(len(imdb[imdb['sentiment'] == 'positive'])) 
print(len(imdb[imdb['sentiment'] == 'negative']))

#Sentiments encoding positive as 1 and negative as 0 using preprocessing.LabelEncoder() from sklearn 
from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()
data['sentiment'] = label_encoder.fit_transform(data['sentiment'])
data.head()

#Convert the dataframe to numppy array
sentiments=(np.array(data['sentiment']))

sentiments

#First stage of pre-processing
imdb['review'] = imdb['review'].apply(lambda x: x.lower()) #Convert all the charecters to lower case
imdb['review'] = imdb['review'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))  #remove all alpha numeric charecters including puncuations and replace them with blank

imdb['review'] #Reviews after first stage of pre-processing

#Tokenization and padding
for index,row in imdb.iterrows(): 
    row[0] = row[0].replace('rt',' ')
    max_features = 2000 #Set the maximum length of vocabulary to 2000 meaning top 2000 words will be in the vocabulary
token = Tokenizer(num_words=max_features, split=' ') #Using Tokenizer create a class
token.fit_on_texts(imdb['review'].values) #Fit the text to the Tokenizer class created
word_index = token.word_index

reviews = token.texts_to_sequences(data['review'].values) #Convert each words of the text to its integer representation
reviews = pad_sequences(reviews,maxlen=100) #Pad the sequence and set maximum sequence length to 100

print(reviews[1].shape)

#Splitting data into train, validation and test with ratio of 50,30,20
X_train, X_val1, Y_train, Y_val1 = train_test_split(reviews, sentiments, test_size=0.50, random_state=50)
X_val, X_test, Y_val, Y_test = train_test_split(X_val1, Y_val1, test_size=0.40, random_state=50)  

print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)
print(X_val.shape,Y_val.shape)

"""### **Single Layer LSTM**"""

#single layer lstm model build
max_features = 2000 #Maximum vocabulary size
embed_dim = 128 #Embedding dimension for embedding layer

single_lstm_build = Sequential() #Building a sequential model
single_lstm_build.add(Embedding(max_features, embed_dim,input_length = reviews.shape[1])) #Embedding layer 
single_lstm_build.add(LSTM(100, return_sequences=False, recurrent_dropout=0.1,input_shape=(100,))) #LSTM layer with 100 neurons and with input shape set
single_lstm_build.add(Dense(7, activation = "relu")) #First dense layer with 7 neurons and activation relu
single_lstm_build.add(Dropout(0.6)) #60% droput to previous dense layer to avoid overfitting 
single_lstm_build.add(Dense(1,activation='sigmoid')) #Final dense layer with 1 neuron to give output of 0 or 1 and sigmoid activation function
single_lstm_build.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) #Model compilation
print(single_lstm_build.summary()) #Print model summary

#Input batch size set to 100 and model is fit with training and validation set.
batch_size = 100 
single_lstm = single_lstm_build.fit(X_train, Y_train, epochs = 5, batch_size=batch_size,validation_data=(X_val, Y_val))

#Model eveluation on test data and printing model accuracy and loss
print("\n Test loss and accuracy ")
single_lstm_test = single_lstm_build.evaluate(X_test, Y_test, batch_size=256)
print("Loss:", single_lstm_test[0])
print("Accuracy:", single_lstm_test[1])

#Predicting class of test set
prediction_single_lstm = single_lstm_build.predict_classes(X_test)
prediction_single_lstm

#Train validate accuracy plots using functions defined
accuracy_plot(single_lstm)
loss_plot(single_lstm)

"""### **Multi-Layer LSTM**"""

#multi Layer LSTM
max_features = 2000 #Maximum vocabulary size
embed_dim = 200 #Embedding dimension for embedding layer

multi_lstm_build = Sequential() #Building a sequential model
multi_lstm_build.add(Embedding(max_features, embed_dim,input_length = reviews.shape[1])) #Embedding layer 
multi_lstm_build.add(LSTM(100, return_sequences=True,recurrent_dropout=0.2,input_shape=(100,))) #LSTM layer with 100 neurons and return_sequences set to true as output should be 3D to be passed as input t next LSTM layer
multi_lstm_build.add(LSTM(40)) #Second LSTM layer with 40 neurons
multi_lstm_build.add(Dense(7, activation = "relu")) #First dense layer with 7 neurons and activation relu
multi_lstm_build.add(Dropout(0.5)) #50% droput to previous dense layer to avoid overfitting 
multi_lstm_build.add(Dense(1,activation='sigmoid')) #Final dense layer with 1 neuron to give output of 0 or 1 and sigmoid activation function
multi_lstm_build.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) #Model compilation
print(multi_lstm_build.summary()) #Print model summary

#Input batch size set to 100 and model is fit with training and validation set and early stopping is used
batch_size = 100
multi_layer_lstm = multi_lstm_build.fit(X_train, Y_train, epochs = 5, batch_size=batch_size,validation_data=(X_val, Y_val),callbacks=[stopping])

#Model eveluation on test data and printing model accuracy and loss
print("\n# Test loss and accuracy ")
multi_lstm_test = multi_lstm_build.evaluate(X_test, Y_test, batch_size=256)
print("Loss:", multi_lstm_test[0])
print("Accuracy:", multi_lstm_test[1])

#Predicting class of test set
prediction_multi_lstm = multi_lstm_build.predict_classes(X_test)
prediction_multi_lstm

#Train validate accuracy plots
accuracy_plot(multi_layer_lstm)
loss_plot(multi_layer_lstm)

"""### **Simple RNN Model**"""

max_fatures = 2000 #Maximum vocabulary size
embed_dim = 128 #Embedding dimension for embedding layer

rnn_build = Sequential() #Building a sequential model
rnn_build.add(Embedding(max_fatures, embed_dim,input_length = reviews.shape[1])) #Embedding layer 
rnn_build.add(SimpleRNN(100, input_shape=(100,))) #Simple RNN layer with 100 neurons and with input shape set
rnn_build.add(Dense(8, activation = "relu")) #First dense layer with 8 neurons and activation relu
rnn_build.add(Dropout(0.6)) #60% droput to previous dense layer to avoid overfitting 
rnn_build.add(Dense(1,activation='sigmoid')) #Final dense layer with 1 neuron to give output of 0 or 1 and sigmoid activation function
rnn_build.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy']) #Model compilation
print(rnn_build.summary())  #Print model summary

#Input batch size set to 100 and model is fit with training and validation set.
batch_size = 100
simple_rnn = rnn_build.fit(X_train, Y_train, epochs = 5, batch_size=batch_size,validation_data=(X_val, Y_val))

#Model eveluation on test data and printing model accuracy and loss
print("\n# Test loss and accuracy ")
simple_rnn_test = rnn_build.evaluate(X_test, Y_test, batch_size=256)
print("Loss:", simple_rnn_test[0])
print("Accuracy:", simple_rnn_test[1])

#Predicting class of test set
prediction_rnn = rnn_build.predict_classes(X_test)
prediction_rnn

accuracy_plot(simple_rnn)
loss_plot(simple_rnn)

from keras.layers import SpatialDropout1D

"""### **Multi-Channel CNN with kernal sizes 2 and 5**"""

#Kernal size=2 and 5
max_features = 2000 #Maximum vocabulary size
embed_dim = 128 #Embedding dimension for embedding layer

#Building functional model
# First channel
inputs_shape1 = Input(shape=(100,)) #Define input shape
embedding1 = Embedding(max_features, embed_dim)(inputs_shape1) #Embedding layer 
conv3 = Conv1D(filters=20, kernel_size=2, activation='relu')(embedding1) #1D Convolutional layer with 20 filters and kernel size of 2
dropout_1 = Dropout(0.5)(conv3) #50% droput applied to Conv1D layer to avoid overfitting
max_pool_1 = MaxPooling1D(pool_size=2)(dropout_1) #Max pooling applied to downsample 
flat_layer_1 = Flatten()(max_pool_1) #Flatten layer to convert 3D output to 2D

# Second channel
inputs_shape2 = Input(shape=(100,)) #Define input shape
embedding2 = Embedding(max_features, embed_dim)(inputs_shape2) #Embedding layer 
conv5 = Conv1D(filters=20, kernel_size=5, activation='relu')(embedding2) #1D Convolutional layer with 20 filters and kernel size of 5
dropout_2 = Dropout(0.5)(conv5) #50% droput applied to Conv1D layer to avoid overfitting
max_pool_2 = MaxPooling1D(pool_size=2)(dropout_2) #Max pooling applied to downsample 
flat_layer_2 = Flatten()(max_pool_2) #Flatten layer to convert 3D output to 2D

#Merging the outputs of two channels 
merge_layer = concatenate([flat_layer_1, flat_layer_2])
dense = Dense(8, activation='relu')(merge_layer) #Merged output which is 2D is passed to dense layer with 8 neurons
output_layer = Dense(1, activation='sigmoid')(dense) #Final dense layer with1 neuror for output
model_multi_cnn = Model(inputs=[inputs_shape1, inputs_shape2], outputs=output_layer) #Build a functional model bt taking two inputs from two channels and using all the layers of both the channels
model_multi_cnn.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy']) #Complie the model
model_multi_cnn.summary() #Print model summary

#Input batch size set to 60 and model is fit with training and validation set. Since there are two channels two training and validation set are used one for each channels
batch_size=60
cnn_9 = model_multi_cnn.fit([X_train,X_train], array(Y_train), epochs = 5, batch_size=batch_size,validation_data=([X_val, X_val],array(Y_val)), callbacks=[stopping])

#Model eveluation on test data and printing model accuracy and loss
print("\n# Test loss and accuracy ")
conv_9_test = model_multi_cnn.evaluate([X_test,X_test],array( Y_test), batch_size=100)
print("Loss:", conv_9_test[0])
print("Accuracy:", conv_9_test[1])

#Predicting class of test set
prediction_conv = model_multi_cnn.predict([X_test,X_test])
prediction_conv

#Plot model accuracy and loss
accuracy_plot(cnn_9)
loss_plot(cnn_9)

"""### **Multi-Channel CNN+LSTM model with kernel sizes 2 and 5**"""

max_features = 2000 #Maximum vocabulary size
embed_dim = 128 #Embedding dimension for embedding layer

#Building functional model
# First channel

inputs_shape1 = Input(shape=(100,)) #Define input shape
embedding1 = Embedding(max_features, embed_dim)(inputs_shape1) #Embedding layer 
conv3 = Conv1D(filters=20, kernel_size=2, activation='relu',padding='same')(embedding1) #1D Convolutional layer with 20 filters and kernel size of 2
dropout_1 = Dropout(0.5)(conv3) #50% droput applied to Conv1D layer to avoid overfitting
max_pool_1 = MaxPooling1D(pool_size=2)(dropout_1) #Max pooling applied to downsample 

# Second channel
inputs_shape2 = Input(shape=(100,)) #Define input shape
embedding2 = Embedding(max_features, embed_dim)(inputs_shape2) #Embedding layer 
conv5 = Conv1D(filters=20, kernel_size=5, activation='relu',padding='same')(embedding2) #1D Convolutional layer with 20 filters and kernel size of 5
dropout_2 = Dropout(0.5)(conv5) #50% droput applied to Conv1D layer to avoid overfitting
max_pool_2 = MaxPooling1D(pool_size=2)(dropout_2) #Max pooling applied to downsample 

#Merging the outputs of two channels 
merge_layer = concatenate([max_pool_1, max_pool_2])
lstm_layer = LSTM(100,recurrent_dropout=0.2)(merge_layer) #Merged output is sent to LSTM layer with 100 neuron and 20% recurrent droput applied to reduce overfitting 
dense = Dense(8, activation='relu')(lstm_layer) #Dense layer with 8 neurons
output_layer = Dense(1, activation='sigmoid')(dense) #Final dense layer with1 neuror for output
model_multi_cnn_lstm = Model(inputs=[inputs_shape1, inputs_shape2], outputs=output_layer) #Build a functional model bt taking two inputs from two channels and using all the layers of both the channels
model_multi_cnn_lstm.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy']) #Complie the model
model_multi_cnn_lstm.summary() #Print model summary

#Input batch size set to 60 and model is fit with training and validation set. Since there are two channels two training and validation set are used one for each channels
cnnlstm_9 = model_multi_cnn_lstm.fit([X_train,X_train], array(Y_train), epochs = 5, batch_size=60,validation_data=([X_val, X_val],array(Y_val)),callbacks=[stopping])

#Model eveluation on test data
print("\n Test loss and accuracy ")
cnnlstm_9_test = model_multi_cnn_lstm.evaluate([X_test,X_test],array( Y_test), batch_size=100)
print("Loss:", cnnlstm_9_test[0])
print("Accuracy:", cnnlstm_9_test[1])

#Predicting class of test set
prediction_conv_lstm = model_multi_cnn_lstm.predict([X_test,X_test])
prediction_conv_lstm

#Plot model accuracy and loss
accuracy_plot(cnnlstm_9)
loss_plot(cnnlstm_9)

"""### **Simple Model with Embeddings learnt on fly**"""

max_features = 2000 #Maximum vocabulary size
embed_dim = 128 #Embedding dimension for embedding layer

embedding_model = Sequential() #Building a sequential model
embedding_model.add(Embedding(max_features, embed_dim, input_length=reviews.shape[1])) #Embedding layer 
embedding_model.add(AveragePooling1D()) #Average pooling to downsample
embedding_model.add(Flatten()) #Flatten layer to convert 3D output to 2D
embedding_model.add(Dense(6, activation='relu')) #First dense layer with 6 neurons and activation relu
embedding_model.add(Dropout(0.6)) #60% droput to previous dense layer to avoid overfitting 
embedding_model.add(Dense(1,activation='sigmoid')) #Final dense layer with 1 neuron to give output of 0 or 1 and sigmoid activation function
embedding_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) #Model compilation
embedding_model.summary() #Print model summary

#Input batch size set to 100 and model is fit with training and validated set
emd_model = embedding_model.fit(X_train, Y_train, batch_size=100, epochs=5, validation_data=(X_val, Y_val),callbacks=[stopping])

#Model eveluation on test data and printing model accuracy and loss
print("\n Test loss and accuracy ")
embd_test = embedding_model.evaluate(X_test, Y_test, batch_size=32)
print("Loss:", embd_test[0])
print("Accuracy:", embd_test[1])

#Predicting class of test set
prediction_embedding = embedding_model.predict_classes(X_test)
prediction_embedding

#Plot accuracy and loss of model
accuracy_plot(emd_model)
loss_plot(emd_model)

"""### **Embedings using pre-trained**

***Model using pre-trained embeddings using Swivel from tensorflow hub***
"""

import tensorflow_hub as hub #To create hub layer

#As pre-processing is taken care by the pre-trained embeddings another set of training, validation and test set is created with 50,30,20 slpit using the unprocessed IMDB set
x2_train, x2_val2, y2_train, y2_val2 = train_test_split(imdb['review'], sentiments, test_size=0.50)
x2_val, x2_test, y2_val, y2_test = train_test_split(x2_val2, y2_val2, test_size=0.40)

print(x2_train.shape,y2_train.shape)
print(x2_test.shape,y2_test.shape)
print(x2_val.shape,y2_val.shape)

#Creation of model using hub layer
hub_layer = hub.KerasLayer("https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1", input_shape=[], dtype=tf.string,trainable=True) #Define hub layer to use pre-trained ebeddin from the path provided and the data type is set to tensor string
pre_trained = tf.keras.Sequential() #Building a sequential model
pre_trained.add(hub_layer) #Add the hub layer defined
pre_trained.add(tf.keras.layers.Dropout(0.2)) #20% droput to previous dense layer to avoid overfitting 
pre_trained.add(tf.keras.layers.Dense(16, activation='relu')) #First dense layer with 16 neurons and activation relu
pre_trained.add(tf.keras.layers.Dense(1, activation='sigmoid')) #Final dense layer with 1 neuron to give output of 0 or 1 and sigmoid activation function
pre_trained.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  #Model compilation
pre_trained.summary() #Print model summary

#Input batch size set to 100 and model is fit with training and validated set
swivel = pre_trained.fit(x2_train, y2_train, batch_size=100, epochs=5, validation_data=(x2_val, y2_val),callbacks=[stopping])

#Model eveluation on test data
print("\n Test loss and accuracy ")
pre_test = pre_trained.evaluate(x2_test, y2_test, batch_size=120)
print("Loss:", pre_test[0])
print("Accuracy:", pre_test[1])

#Predicting class of test set
prediction_pre_trained = pre_trained.predict_classes(x2_test)
prediction_pre_trained

"""### **Model Saving**"""

#Best model was the one with pre-trained embeddings
model_pre_trained = pre_trained.save('swivel_best.h5') #Save the model as HDF5 file
model8 = drive.CreateFile({'Models' : 'swivel_best.h5'}) #Create a file in drive       
model8.SetContentFile('swivel_best.h5') #  Upload the file contents         
model8.Upload()
drive.CreateFile({'id': model8.get('id')}) #Download the file in google drive

#Plot accuracy and loss plots
accuracy_plot(swivel)
loss_plot(swivel)

"""# **PART 2: Working with data**

### **Model created by loading the pre-trained best model saved in drive**
"""

#Import new dataset with 60 rows from drive 
data_2= pd.read_csv('/content/drive/My Drive/IMDB_Part2.csv',sep=';')
data_2.tail(10)

#Sentiments encoding positive as 1 and negative as 0 using preprocessing.LabelEncoder() from sklearn 
from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()
data_2['sentiment'] = label_encoder.fit_transform(data_2['sentiment'])
data_2.head()

sentiment_2=(np.array(data_2['sentiment']))

sentiment_2

#Splitting data into train, validation with ratio of 70,30
x3_train, x3_val3, y3_train, y3_val3 = train_test_split(data_2['review'], sentiment_2, test_size=0.30)

print(x3_train.shape,y3_train.shape)
print(x3_val3.shape,y3_val3.shape)

#Loading model
from keras.models import load_model
#Load the pre-trained model by mentioning the path to the file and set the custom_objects as the hub layer was custom created layer and no8t inbuit layer of keras
model_trained=tf.keras.models.load_model('/content/drive/My Drive/swivel_best.h5',custom_objects={'KerasLayer':hub.KerasLayer})
# summarize model.
model_trained.summary()

#Input batch size set to 100 and model is fit with training and validated set
swivel_2 = model_trained.fit(x3_train, y3_train, batch_size=100, epochs=10, validation_data=(x3_val3,y3_val3))

#Model eveluation on test data
print("\n Test loss and accuracy ")
pre_test = model_trained.evaluate(x3_val3, y3_val3)
print("Loss:", pre_test[0])
print("Accuracy:", pre_test[1])

#Predicting class of test set
prediction_model_trained= model_trained.predict_classes(x3_val3)
prediction_model_trained

#Plot model accuracy and loss
accuracy_plot(swivel_2)
loss_plot(swivel_2)

"""### **Model Build from scratch using same architecture of the loaded model**"""

#Creation of model using hub layer
hub_layer = hub.KerasLayer("https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1", input_shape=[], dtype=tf.string,trainable=True)  #Define hub layer to use pre-trained ebeddin from the path provided and the data type is set to tensor string
pre_trained_2 = tf.keras.Sequential()  #Building a sequential model
pre_trained_2.add(hub_layer)  #Add the hub layer defined
pre_trained_2.add(tf.keras.layers.Dropout(0.2)) #20% droput to previous dense layer to avoid overfitting 
pre_trained_2.add(tf.keras.layers.Dense(8, activation='relu')) #First dense layer with 16 neurons and activation relu
pre_trained_2.add(tf.keras.layers.Dense(1, activation='sigmoid')) #Final dense layer with 1 neuron to give output of 0 or 1 and sigmoid activation function
pre_trained_2.summary() #Model compilation

#Model compilation
pre_trained_2.compile(optimizer='adam',loss='binary_crossentropy', metrics=['accuracy'])

#Input batch size set to 100 and model is fit with training and validated set
swivel_3 = pre_trained_2.fit(x3_train, y3_train, batch_size=100, epochs=10, validation_data=(x3_val3,y3_val3))

#Model eveluation on test data
print("\n Test loss and accuracy ")
pre_test = pre_trained_2.evaluate(x3_val3, y3_val3)
print("Loss:", pre_test[0])
print("Accuracy:", pre_test[1])

#Predicting class of test set
prediction_pre_trained_2= pre_trained_2.predict_classes(x3_val3)
prediction_pre_trained_2

#Plot model accuracy and loss
accuracy_plot(swivel_3)
loss_plot(swivel_3)

#Best model was the one with pre-trained model
model_part2 = model_trained.save('Model_Part_2_Best_1.h5') #Save the model as HDF5 file
model_9 = drive.CreateFile({'title' : 'Model_Part_2_Best_1.h5'}) #Create a file in drive       
model_9.SetContentFile('Model_Part_2_Best_1.h5')  #Upload the file contents               
model_9.Upload()
drive.CreateFile({'id': model_9.get('id')}) #Download the file in google drive
