# -*- coding: utf-8 -*-
"""Part3 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1guwfk999ZgSnV7d8ArIHxW0JuEfW3RBF

# **Part 2: Writing your own Reviews**

**Google Drive has to be mounted to this colab ntebook to access the dataset**
"""

#Command to Mount google drive to this notebook
from google.colab import drive
drive.mount('/content/drive')

"""**Load the Required libraries**"""

#IMporting requried libraries
import tensorflow as tf #To build deep learning models
from tensorflow import keras #To build deep learning layers
from keras.preprocessing.text import Tokenizer #Tokenize the reviews
import pandas as pd #pandas to read csv file 
import numpy as np #numpy to import array function
from sklearn.feature_extraction.text import CountVectorizer
from keras.preprocessing.sequence import pad_sequences #Pad the reviews 
from keras.models import Sequential #Build keras sequential model
from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D,SimpleRNN,Conv1D,GlobalMaxPooling1D,TimeDistributed #Build various different layers
from keras.layers import Dropout, Activation, Flatten, GlobalAveragePooling1D,AveragePooling1D,MaxPooling1D #Build variour layers and add regularizations
from sklearn.model_selection import train_test_split #Split dataset to training, validation and test set
from keras.utils.np_utils import to_categorical
from tensorflow.python import keras as K
import re #To create regular expressions

#Moduels from pydrive to save and load models to google drive
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive 
from google.colab import auth 
from oauth2client.client import GoogleCredentials

#Google drive is authenticated to have an access to store models to the drive
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

"""**Import the dataset from Google drive**"""

#Read file from the path in google drive where it is stored and store it in pandas dataframe
data= pd.read_csv('/content/drive/My Drive/IMDB Dataset.csv')

"""**There are two was in which dataframe is converted to corpus**

**1.Converting dataframe to string directly**
"""

#We can select particular column and do the processing or if we want it we can store in seperate file to know the entire contents of the file
pos=data[data['sentiment']=='positive'] #Select only positive reviews from dataframe and store in seperate dataframe 
neg=data[data['sentiment']=='negative'] #Select only negative reviews from dataframe and store in seperate dataframe 
all_=data['review'] #Select all reviews from dataframe and store in seperate dataframe

#Convert the contents of the dataframe to one single string
pos=' '.join(pos['review'].tolist())
neg=' '.join(neg['review'].tolist())
all_=' '.join(all_['review'].tolist())

"""**2.Storing contents to .txt file**"""

#Functions to store each of the review types in seperate text files
#Text file with positive reviews
def CreateCorpusFromDataFrame(corpusfolder,df):
    for index, r in df.iterrows(): #Iterate through each rows and store row by row into a dataframe
        pos_reviews=r[r['sentiment']=='positive']
        fname1= 'pos_reviews' +'.txt' 
        corpusfile1=open(corpusfolder+'/'+fname1,'a') #Create a file if not exist
        corpusfile1.write(str(pos_reviews)) #Write to the file contents of negative reviews from the dataframe 
        corpusfile1.close()
CreateCorpusFromDataFrame('/content/drive/My Drive',data)

#Text file with neative reviews
def Corpus_neg(folder_namer,df):
    for index, row in df.iterrows(): #Iterate through each rows and store row by row into a dataframe
        neg_reviews=row[row['sentiment']=='negative']
        file_name = 'neg_reviews' +'.txt'
        corpus_file=open(folder_namer+'/'+file_name,'a') #Create a file if not exist
        corpus_file.write(str(neg_reviews)) #Write to the file contents of positive reviews from the dataframe
        corpus_file.close()
Corpus_neg('/content/drive/My Drive',data)

#Text file with all reviews
def Corpus_all(folder_namer,df):
    for index, row in df.iterrows(): #Iterate through each rows and store row by row into a dataframe
        all1_reviews=row['review']
        file_name = 'all1_reviews' +'.txt'
        corpus_file=open(folder_namer+'/'+file_name,'a') #Create a file if not exist
        corpus_file.write(str(all1_reviews)) #Write to the file contents of all reviews from the dataframe
        corpus_file.close()
Corpus_all('/content/drive/My Drive',data)

#Reading the files from drive to seperate variables as string to perform text generation tasks
with open('/content/drive/My Drive/pos_reviews.txt', 'r') as pos:
  pos_corpus = pos.read()

with open('/content/drive/My Drive/neg_reviews.txt', 'r') as neg:
  neg_corpus = neg.read()

with open('/content/drive/My Drive/all1_reviews.txt', 'r') as all:
  allreviews_corpus = all.read()

#Display first 1000 charecters from the string
  pos_corpus[:1000]
  neg_corpus[:1000]
  allreviews_corpus[:1000]

"""**Function Definations**"""

#Function for text processing and creating tensor datasets
def text_processing(string1):
  text_lower = string1.lower()
  final_string_name= re.sub(r'[^\x00-\x7f]',r'', text_lower) #Convert text characters to lower case and remove all non- ascii characters 
  final_string_name = " ".join(final_string_name.split("<br />")) #Remove <br /> and replace with blank 
  text_processing.vocabulary = sorted(set(string1)) #Create vocabulary of unique characters 
  # Create two dictionaries
  text_processing.char_to_index = {u:i for i, u in enumerate(text_processing.vocabulary)} #Convert characters from vocabulary to integer
  text_processing.index_to_char = np.array(text_processing.vocabulary) #Convert integers from to characters
  word_to_int = np.array([ text_processing.char_to_index[c] for c in string1]) #Convert the input text to integer representaion
  sequence_length = 80 #Set maximum sequece length for the input 
  examples_per_epoch = len(string1)//(sequence_length+1)
  # Create TensorFlow dataset object and split into the batches of 80+1.
  char_dataset = tf.data.Dataset.from_tensor_slices(word_to_int)
  text_processing.sequences = char_dataset.batch(sequence_length+1, drop_remainder=True)

#LSTM model defination
def model(vocab,batch_size):
  model_lstm = tf.keras.Sequential() #Build sequential model
  model_lstm.add(tf.keras.layers.Embedding(len(vocab), embedding_dim, batch_input_shape=[batch_size, None])) #Embedding layer with embedding dimension 256 and input dimension same as length of vocabulary
  model_lstm.add(tf.keras.layers.LSTM(512, return_sequences= True, stateful=True, recurrent_initializer='glorot_uniform')) #Buld LSTM layer with 512 neurons 
  model_lstm.add(tf.keras.layers.Dense(len(vocab))) #Final dense layer with same number of neurons as the length of vocabulary
  model_lstm.summary() #Model summary
  return model_lstm

#FUnction for text generation
def generate_text(model, start_string):
  # Number of characters to generate
  max_num_characters = 1000 #Maximum number of text to be generated 
  text_vectorized = [text_processing.char_to_index[s] for s in start_string] #Converting first string passed in the function to integer 
  text_vectorized = tf.expand_dims(text_vectorized, 0) #Reshape the vectorized text to 2D
  # Empty string to store our results
  final_text = []
  
  temperature = 1.0 #Temperature set to 1.0 to print out text without errors like spellin mistakes
  model.reset_states() #Reset the model states and batch size will be 1
  text_index = [] #Empty list to store integer representation
  #Model predicts character by character and the output character is passed as input to the same model
  for i in range(max_num_characters):
    predicted_text = model(text_vectorized) #use model to predict next charecter after the first input string
    predicted_text = tf.squeeze(predicted_text, 0)
    predicted_text = predicted_text / temperature # using a categorical distribution to predict the character returned by the model
    predicted_id = tf.random.categorical(predicted_text, num_samples=1)[-1,0].numpy()
    # Final predicted character is passed as input to the same model
    text_vectorized = tf.expand_dims([predicted_id], 0)
    text_index.append(predicted_id)
    final_text.append(text_processing.index_to_char[predicted_id]) #Convert the final integer representaion to characters by looking at the character dictionary
  return (start_string + ''.join(final_text)) #Return the first word along with the predicted texts

#Calculate perplexity using sparse_categorical_crossentropy
def perplexity(y_true, y_pred):
  
    cross_entropy = K.backend.mean(K.backend.sparse_categorical_crossentropy(y_true, y_pred)) #Using keras backend API k calculate sparse_categorical_crossentropy 
    perplexity = K.backend.exp(cross_entropy) #calculate perplexity by take exponint to the power of entropy
    return perplexity

#Loss function
def loss(labels, logits):
  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

"""**Text genration model using positive reviews**"""

pos_preprocessed=text_processing(pos_corpus) #Generate tensor object

#Split the sequnce into input and target text 
def split_input_target(chunk):
  input_text = chunk[:-1]
  target_text = chunk[1:]
  return input_text, target_text  

dataset = text_processing.sequences.map(split_input_target)

dataset

#Print the input and tagert text
for input_example, target_example in  dataset.take(1):
  print ('Input data: ', repr(''.join(text_processing.index_to_char[input_example])))
  print ('Target data:', repr(''.join(text_processing.index_to_char[target_example])))

tf.keras.backend.clear_session()

batch_size = 100 #Define batch size to create final tensor dataset
buffer_size = 10000 #Set buffer size to shuffel the tensor dataset

#Final dataset
dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True )

dataset

embedding_dim = 256 #Embedding dimension for embedding layer

pos_lstm=model(text_processing.vocabulary,100) #Build LSTM model using model() function

import os

temp_folder='/content/drive/My Drive/Models' #Path to where the checkpoint folder to be created
checkpoint_directory = temp_folder+'_training_checkpoints_model_111' #Directory of checkpoints with checkpoint folder
#Check if the folder is not already created
import shutil
try:
  shutil.rmtree(checkpoint_directory)
except:
  print("directory not used yet.")
# Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_directory, "ckpt_")
#create callbacks to monitor loss and save the weights 
call_back=tf.keras.callbacks.ModelCheckpoint(
  filepath=checkpoint_prefix,
  monitor='loss',
  save_weights_only=True,
  save_best_only=True)

pos_lstm.compile(optimizer='adam', loss=loss, metrics=[perplexity]) #compileing the model to measure perplexity

history = pos_lstm.fit(dataset, epochs=3,callbacks=[call_back],verbose=1)

#BUild another model with similar architecture and load the weigths from pre-trained model
model_lstm2 = tf.keras.Sequential()
model_lstm2.add(tf.keras.layers.Embedding(len(text_processing.vocabulary), embedding_dim,batch_input_shape=[1, None]))
model_lstm2.add(tf.keras.layers.LSTM(512,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))
model_lstm2.add(tf.keras.layers.Dense(len(text_processing.vocabulary)))
model_lstm2.load_weights(tf.train.latest_checkpoint(checkpoint_directory)) #Load the weights of the latest checkpoint that performed best
model_lstm2.build(tf.TensorShape([1, None])) #Build the model with batch size 1

model_lstm2.summary()

#Generate text using generate_text()
print(generate_text(model_lstm2, start_string= u"The "))

print(generate_text(model_lstm2, start_string= u"This movie "))

print(generate_text(model_lstm2, start_string= u"is this a "))

print(generate_text(model_lstm2, start_string= u"movie was very "))

print(generate_text(model_lstm2, start_string= u"highly rated but"))

print(generate_text(model_lstm2, start_string= u"if at all this was "))

"""***Negative Reviews***"""

neg_preprocessed=text_processing(neg_corpus) #Generate tensor object

#Split the sequnce into input and target text 
def split_input_target_neg(chunk):
  input_text_neg = chunk[:-1]
  target_text_neg = chunk[1:]
  return input_text_neg, target_text_neg  

data_set = text_processing.sequences.map(split_input_target_neg)

data_set

#Print the input and tagert text
for input_example, target_example in  data_set.take(1):
  print ('Input data: ', repr(''.join(text_processing.index_to_char[input_example.numpy()])))
  print ('Target data:', repr(''.join(text_processing.index_to_char[target_example.numpy()])))

tf.keras.backend.clear_session()

#Final dataset
dataset1 = data_set.shuffle(buffer_size).batch(batch_size, drop_remainder=True )

dataset1

neg_model=model(text_processing.vocabulary,100) #Build LSTM model using model() function

temp_folder='/content/drive/My Drive/Models' #Path to where the checkpoint folder to be created

checkpoint_directory = temp_folder+'_training_checkpoints_model_22' #Directory of checkpoints with checkpoint folder
#Check if the folder is not already created
import shutil
try:
  shutil.rmtree(checkpoint_directory)
except:
  print("directory not used yet.")
# Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_directory, "ckpt_")
#create callbacks to monitor loss and save the weights
call_back=tf.keras.callbacks.ModelCheckpoint(
  filepath=checkpoint_prefix,
  monitor='loss',
  save_weights_only=True,
  save_best_only=True)

neg_model.compile(optimizer='adam', loss=loss, metrics=[perplexity])  #compileing the model to measure perplexity

neg_train = neg_model.fit(dataset1, epochs=3,batch_size= 250, batch_verbose=1,callbacks=[call_back])

#BUild another model with similar architecture and load the weigths from pre-trained model
neg_model2 = tf.keras.Sequential()
neg_model2.add(tf.keras.layers.Embedding(len(text_processing.vocabulary), embedding_dim,batch_input_shape=[1, None]))
neg_model2.add(tf.keras.layers.LSTM(512,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))
neg_model2.add(tf.keras.layers.Dense(len(text_processing.vocabulary)))
neg_model2.load_weights(tf.train.latest_checkpoint(checkpoint_directory))  #Load the weights of the latest checkpoint that performed best
neg_model2.build(tf.TensorShape([1, None]))  #Build the model with batch size 1

#Generate text using generate_text()
print(generate_text(neg_model2, start_string= u"It is"))

print(generate_text(neg_model2, start_string= u"enjoying the "))

print(generate_text(neg_model2, start_string= u"But it's a film"))

print(generate_text(neg_model2, start_string= u"The"))

print(generate_text(neg_model2, start_string= u"negative"))

"""***All reviews***"""

allreviews_preprocessed=text_processing(allreviews_corpus) #Generate tensor object

#Split the sequnce into input and target text 
def split_input_target_all(chunk):
  input_text_all = chunk[:-1]
  target_text_all = chunk[1:]
  return input_text_all, target_text_all  

data_set2 = text_processing.sequences.map(split_input_target_all)

data_set2

#Print the input and tagert text
for input_example, target_example in  data_set2.take(1):
  print ('Input data: ', repr(''.join(text_processing.index_to_char[input_example.numpy()])))
  print ('Target data:', repr(''.join(text_processing.index_to_char[target_example.numpy()])))

#Final dataset  
all_dataset = data_set2.shuffle(buffer_size).batch(batch_size, drop_remainder=True )

all_dataset

allreviews_model=model(text_processing.vocabulary,100) #Build LSTM model using model() function

temp_folder='/content/drive/My Drive/Models' #Path to where the checkpoint folder to be created

checkpoint_directory = temp_folder+'training_checkpoints_model_33' #Directory of checkpoints with checkpoint folder
#Check if the folder is not already created
import shutil
try:
  shutil.rmtree(checkpoint_directory)
except:
  print("directory not used yet.")
# Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_directory, "ckpt_")
#create callbacks to monitor loss and save the weights 
call_back=tf.keras.callbacks.ModelCheckpoint(
  filepath=checkpoint_prefix,
  monitor='loss',
  save_weights_only=True,
  save_best_only=True)

allreviews_model.compile(optimizer='adam', loss=loss,metrics=[perplexity]) #compileing the model to measure perplexity

allreviews_train = allreviews_model.fit(all_dataset, epochs=3,callbacks=[call_back],verbose=1)

#BUild another model with similar architecture and load the weigths from pre-trained model
allreviews_model2 = tf.keras.Sequential()
allreviews_model2.add(tf.keras.layers.Embedding(len(text_processing.vocabulary), embedding_dim,batch_input_shape=[1, None]))
allreviews_model2.add(tf.keras.layers.LSTM(512,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))
allreviews_model2.add(tf.keras.layers.Dense(len(text_processing.vocabulary)))
allreviews_model2.load_weights(tf.train.latest_checkpoint(checkpoint_directory))  #Load the weights of the latest checkpoint that performed best
allreviews_model2.build(tf.TensorShape([1, None]))  #Build the model with batch size 1

allreviews_model2.summary()

#Generate text using generate_text()
print(generate_text(allreviews_model2, start_string=u"The "))

print(generate_text(allreviews_model2, start_string=u"this movie"))

print(generate_text(allreviews_model2, start_string=u"it was a horrible "))

print(generate_text(allreviews_model2, start_string=u"screenplay was very slow "))

print(generate_text(allreviews_model2, start_string=u"camera work "))

model_part3 = allreviews_model2.save('allreviews_model2_1.h5') #Save the model as HDF5 file
model_11 = drive.CreateFile({'Models' : 'allreviews_model2_1.h5'}) #Create a file in drive    
model_11.SetContentFile('allreviews_model2_1.h5') #  Upload the file contents    
model_11.Upload()
drive.CreateFile({'id': model_11.get('id')}) #Download the file in google drive

"""**N-Gram Statistical Model for Language Generation**

**Positive Review N-Gram Model**
"""

pip install -U nltk==3.4 #Installing nltk 3.4 as MLE is not avialable in previous version

#Importing required libraries
import nltk
from nltk.util import ngrams #Creates ngrams 
from nltk.lm import MLE #build MLE model
from nltk import word_tokenize, sent_tokenize  #word tokenization and sentence tokenization
from nltk.lm.preprocessing import padded_everygram_pipeline #Padding sentences

nltk.download('punkt')

#First text is aplit into sentence sequences and from each sentence words are split and characters are changed to lower case 
pos_text = [list(map(str.lower, word_tokenize(raw_text))) 
                  for raw_text in sent_tokenize(pos_corpus)]

#Print n-grams for the text using n=1,2 and 3
print(list(ngrams(pos_text[0], n=1)))
print(list(ngrams(pos_text[0], n=2)))
print(list(ngrams(pos_text[0], n=3)))

#Create vocab and create trigram
train, vocab = padded_everygram_pipeline(3, pos_text)

#Create model for trigram and fit the model using train text
model = MLE(3)
model.fit(train, vocab)

#Generating text by specipying the number of words to generate and also seed is set
pos_list = model.generate(15, random_seed=5)
print(' '.join(word for word in pos_list))

pos_list = model.generate(10, random_seed=2)
print(' '.join(word for word in pos_list))

pos_list = model.generate(25, random_seed=8)
print(' '.join(word for word in pos_list))

pos_list = model.generate(12, random_seed=10)
print(' '.join(word for word in pos_list))

pos_list = model.generate(8, random_seed=11)
print(' '.join(word for word in pos_list))

"""**Negative Review N-Gram Model**"""

#First text is aplit into sentence sequences and from each sentence words are split and characters are changed to lower case 
text_tokenized = [list(map(str.lower, word_tokenize(raw_text))) 
                for raw_text in sent_tokenize(neg_corpus)]

#Print n-grams for the text using n=1,2 and 3
print(list(ngrams(text_tokenized[0], n=1)))
print(list(ngrams(text_tokenized[0], n=2)))
print(list(ngrams(text_tokenized[0], n=3)))

#Create vocab and create trigram
from nltk.lm.preprocessing import padded_everygram_pipeline
train_neg, vocab_neg = padded_everygram_pipeline(3, text_tokenized)

#Create model for trigram and fit the model using train text
model_neg = MLE(3)
model_neg.fit(train_neg, vocab_neg)

#Generating text by specipying the number of words to generate and also seed is set
neg_list = model_neg.generate(15, random_seed=2)
print(' '.join(word for word in neg_list))

neg_list = model_neg.generate(10, random_seed=3)
print(' '.join(word for word in neg_list))

neg_list = model_neg.generate(25, random_seed=8)
print(' '.join(word for word in neg_list))

neg_list = model_neg.generate(12, random_seed=10)
print(' '.join(word for word in neg_list))

neg_list = model_neg.generate(8, random_seed=5)
print(' '.join(word for word in neg_list))

"""**All Review N-Gram Model**"""

#First text is aplit into sentence sequences and from each sentence words are split and characters are changed to lower case 
text_tokenized_all = [list(map(str.lower, word_tokenize(raw_text))) 
                for raw_text in sent_tokenize(allreviews_corpus)]

#Print n-grams for the text using n=1,2 and 3
print(list(ngrams(text_tokenized_all[0], n=1)))
print(list(ngrams(text_tokenized_all[0], n=2)))
print(list(ngrams(text_tokenized_all[0], n=3)))

#Create vocab and create trigram
train_all, vocab_all = padded_everygram_pipeline(3, text_tokenized_all)

#Create model for trigram and fit the model using train text
model_all = MLE(3)
model_all.fit(train_all, vocab_all)

#Generating text by specipying the number of words to generate and also seed is set
all_list = model_all.generate(15, random_seed=2)
print(' '.join(word for word in all_list))

all_list = model_all.generate(10, random_seed=3)
print(' '.join(word for word in all_list))

all_list = model_all.generate(25, random_seed=8)
print(' '.join(word for word in all_list))

all_list = model_all.generate(12, random_seed=10)
print(' '.join(word for word in all_list))

all_list = model_all.generate(8, random_seed=5)
print(' '.join(word for word in all_list))
